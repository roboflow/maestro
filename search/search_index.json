{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hello","title":"\ud83d\udc4b hello","text":"<p>Multimodal-Maestro gives you more control over large multimodal models to get the outputs you want. With more effective prompting tactics, you can get multimodal models to do tasks you didn't know (or think!) were possible. Curious how it works? Try our HF space!</p> <p>\ud83d\udea7 The project is still under construction and the API is prone to change.</p>"},{"location":"#install","title":"\ud83d\udcbb install","text":"<p>\u26a0\ufe0f Our package has been renamed to <code>maestro</code>. Install package in a 3.11&gt;=Python&gt;=3.8 environment.</p> <pre><code>pip install maestro\n</code></pre>"},{"location":"#contribution","title":"\ud83e\uddb8 contribution","text":"<p>We would love your help in making this repository even better! If you noticed any bug, or if you have any suggestions for improvement, feel free to open an issue or submit a pull request.</p>"},{"location":"get_started/","title":"Get Started","text":""},{"location":"get_started/#under-construction","title":"\ud83d\udea7 under construction","text":""},{"location":"lmms/","title":"LMMs","text":""},{"location":"lmms/#gpt-4-vision","title":"GPT-4 Vision","text":"<p>Sends an image and a textual prompt to the OpenAI API and returns the API's textual response.</p> <p>This function integrates an image with a user-defined prompt to generate a response using OpenAI's API.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for authenticating requests to the OpenAI API.</p> required <code>image</code> <code>ndarray</code> <p>The image to be sent to the API. used in OpenCV.</p> required <code>prompt</code> <code>str</code> <p>The textual prompt to accompany the image in the API request.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The textual response from the OpenAI API based on the input image and prompt.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an error in encoding the image or if the API response contains an error.</p> Source code in <code>maestro/lmms/gpt4.py</code> <pre><code>def prompt_image(api_key: str, image: np.ndarray, prompt: str) -&gt; str:\n    \"\"\"Sends an image and a textual prompt to the OpenAI API and returns the API's textual\n    response.\n\n    This function integrates an image with a user-defined prompt to generate a response\n    using OpenAI's API.\n\n    Parameters:\n        api_key (str): The API key for authenticating requests to the OpenAI API.\n        image (np.ndarray): The image to be sent to the API.\n            used in OpenCV.\n        prompt (str): The textual prompt to accompany the image in the API request.\n\n    Returns:\n        str: The textual response from the OpenAI API based on the input image and\n            prompt.\n\n    Raises:\n        ValueError: If there is an error in encoding the image or if the API response\n            contains an error.\n    \"\"\"\n    headers = compose_headers(api_key=api_key)\n    payload = compose_payload(image=image, prompt=prompt)\n    response = requests.post(url=API_URL, headers=headers, json=payload).json()\n\n    if \"error\" in response:\n        raise ValueError(response[\"error\"][\"message\"])\n    return response[\"choices\"][0][\"message\"][\"content\"]\n</code></pre>"},{"location":"markers/","title":"Markers","text":""},{"location":"markers/#segment-anything","title":"Segment Anything","text":"<p>A class for performing image segmentation using a specified model.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>The device to run the model on (e.g., 'cpu', 'cuda').</p> <code>'cpu'</code> <code>model_name</code> <code>str</code> <p>The name of the model to be loaded. Defaults to 'facebook/sam-vit-huge'.</p> <code>'facebook/sam-vit-huge'</code> Source code in <code>maestro/markers/sam.py</code> <pre><code>class SegmentAnythingMarkGenerator:\n    \"\"\"A class for performing image segmentation using a specified model.\n\n    Parameters:\n        device (str): The device to run the model on (e.g., 'cpu', 'cuda').\n        model_name (str): The name of the model to be loaded. Defaults to\n            'facebook/sam-vit-huge'.\n    \"\"\"\n\n    def __init__(self, device: str = \"cpu\", model_name: str = \"facebook/sam-vit-huge\") -&gt; None:\n        self.model = SamModel.from_pretrained(model_name).to(device)\n        self.processor = SamProcessor.from_pretrained(model_name)\n        self.image_processor = SamImageProcessor.from_pretrained(model_name)\n        self.device = device\n        self.pipeline = pipeline(\n            task=\"mask-generation\", model=self.model, image_processor=self.image_processor, device=self.device\n        )\n\n    def generate(self, image: np.ndarray, mask: Optional[np.ndarray] = None) -&gt; sv.Detections:\n        \"\"\"Generate image segmentation marks.\n\n        Parameters:\n            image (np.ndarray): The image to be marked in BGR format.\n            mask: (Optional[np.ndarray]): The mask to be used as a guide for\n                segmentation.\n\n        Returns:\n            sv.Detections: An object containing the segmentation masks and their\n                corresponding bounding box coordinates.\n        \"\"\"\n        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        if mask is None:\n            outputs = self.pipeline(image, points_per_batch=64)\n            masks = np.array(outputs[\"masks\"])\n            return masks_to_marks(masks=masks)\n        else:\n            inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n            image_embeddings = self.model.get_image_embeddings(inputs.pixel_values)\n            masks = []\n            for polygon in sv.mask_to_polygons(mask.astype(bool)):\n                indexes = np.random.default_rng().choice(a=polygon.shape[0], size=5, replace=True)\n                input_points = polygon[indexes]\n                inputs = self.processor(images=image, input_points=[[input_points]], return_tensors=\"pt\").to(\n                    self.device\n                )\n                del inputs[\"pixel_values\"]\n                outputs = self.model(image_embeddings=image_embeddings, **inputs)\n                mask = self.processor.image_processor.post_process_masks(\n                    masks=outputs.pred_masks.cpu().detach(),\n                    original_sizes=inputs[\"original_sizes\"].cpu().detach(),\n                    reshaped_input_sizes=inputs[\"reshaped_input_sizes\"].cpu().detach(),\n                )[0][0][0].numpy()\n                masks.append(mask)\n            masks = np.array(masks)\n            return masks_to_marks(masks=masks)\n</code></pre>"},{"location":"markers/#maestro.markers.sam.SegmentAnythingMarkGenerator.generate","title":"<code>generate(image, mask=None)</code>","text":"<p>Generate image segmentation marks.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The image to be marked in BGR format.</p> required <code>mask</code> <code>Optional[ndarray]</code> <p>(Optional[np.ndarray]): The mask to be used as a guide for segmentation.</p> <code>None</code> <p>Returns:</p> Type Description <code>Detections</code> <p>sv.Detections: An object containing the segmentation masks and their corresponding bounding box coordinates.</p> Source code in <code>maestro/markers/sam.py</code> <pre><code>def generate(self, image: np.ndarray, mask: Optional[np.ndarray] = None) -&gt; sv.Detections:\n    \"\"\"Generate image segmentation marks.\n\n    Parameters:\n        image (np.ndarray): The image to be marked in BGR format.\n        mask: (Optional[np.ndarray]): The mask to be used as a guide for\n            segmentation.\n\n    Returns:\n        sv.Detections: An object containing the segmentation masks and their\n            corresponding bounding box coordinates.\n    \"\"\"\n    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    if mask is None:\n        outputs = self.pipeline(image, points_per_batch=64)\n        masks = np.array(outputs[\"masks\"])\n        return masks_to_marks(masks=masks)\n    else:\n        inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n        image_embeddings = self.model.get_image_embeddings(inputs.pixel_values)\n        masks = []\n        for polygon in sv.mask_to_polygons(mask.astype(bool)):\n            indexes = np.random.default_rng().choice(a=polygon.shape[0], size=5, replace=True)\n            input_points = polygon[indexes]\n            inputs = self.processor(images=image, input_points=[[input_points]], return_tensors=\"pt\").to(\n                self.device\n            )\n            del inputs[\"pixel_values\"]\n            outputs = self.model(image_embeddings=image_embeddings, **inputs)\n            mask = self.processor.image_processor.post_process_masks(\n                masks=outputs.pred_masks.cpu().detach(),\n                original_sizes=inputs[\"original_sizes\"].cpu().detach(),\n                reshaped_input_sizes=inputs[\"reshaped_input_sizes\"].cpu().detach(),\n            )[0][0][0].numpy()\n            masks.append(mask)\n        masks = np.array(masks)\n        return masks_to_marks(masks=masks)\n</code></pre>"},{"location":"visualizers/","title":"Visualizers","text":""},{"location":"visualizers/#markvisualizer","title":"MarkVisualizer","text":"<p>A class for visualizing different marks including bounding boxes, masks, polygons, and labels.</p> <p>Parameters:</p> Name Type Description Default <code>line_thickness</code> <code>int</code> <p>The thickness of the lines for boxes and polygons.</p> <code>2</code> <code>mask_opacity</code> <code>float</code> <p>The opacity level for masks.</p> <code>0.1</code> <code>text_scale</code> <code>float</code> <p>The scale of the text for labels.</p> <code>0.6</code> Source code in <code>maestro/visualizers.py</code> <pre><code>class MarkVisualizer:\n    \"\"\"A class for visualizing different marks including bounding boxes, masks, polygons,\n    and labels.\n\n    Parameters:\n        line_thickness (int): The thickness of the lines for boxes and polygons.\n        mask_opacity (float): The opacity level for masks.\n        text_scale (float): The scale of the text for labels.\n    \"\"\"\n\n    def __init__(self, line_thickness: int = 2, mask_opacity: float = 0.1, text_scale: float = 0.6) -&gt; None:\n        self.box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX, thickness=line_thickness)\n        self.mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX, opacity=mask_opacity)\n        self.polygon_annotator = sv.PolygonAnnotator(color_lookup=sv.ColorLookup.INDEX, thickness=line_thickness)\n        self.label_annotator = sv.LabelAnnotator(\n            color=sv.Color.black(),\n            text_color=sv.Color.white(),\n            color_lookup=sv.ColorLookup.INDEX,\n            text_position=sv.Position.CENTER_OF_MASS,\n            text_scale=text_scale,\n        )\n\n    def visualize(\n        self,\n        image: np.ndarray,\n        marks: sv.Detections,\n        with_box: bool = False,\n        with_mask: bool = False,\n        with_polygon: bool = True,\n        with_label: bool = True,\n    ) -&gt; np.ndarray:\n        \"\"\"Visualizes annotations on an image.\n\n        This method takes an image and an instance of sv.Detections, and overlays\n        the specified types of marks (boxes, masks, polygons, labels) on the image.\n\n        Parameters:\n            image (np.ndarray): The image on which to overlay annotations.\n            marks (sv.Detections): The detection results containing the annotations.\n            with_box (bool): Whether to draw bounding boxes. Defaults to False.\n            with_mask (bool): Whether to overlay masks. Defaults to False.\n            with_polygon (bool): Whether to draw polygons. Defaults to True.\n            with_label (bool): Whether to add labels. Defaults to True.\n\n        Returns:\n            np.ndarray: The annotated image.\n        \"\"\"\n        annotated_image = image.copy()\n        if with_box:\n            annotated_image = self.box_annotator.annotate(scene=annotated_image, detections=marks)\n        if with_mask:\n            annotated_image = self.mask_annotator.annotate(scene=annotated_image, detections=marks)\n        if with_polygon:\n            annotated_image = self.polygon_annotator.annotate(scene=annotated_image, detections=marks)\n        if with_label:\n            labels = list(map(str, range(len(marks))))\n            annotated_image = self.label_annotator.annotate(scene=annotated_image, detections=marks, labels=labels)\n        return annotated_image\n</code></pre>"},{"location":"visualizers/#maestro.visualizers.MarkVisualizer.visualize","title":"<code>visualize(image, marks, with_box=False, with_mask=False, with_polygon=True, with_label=True)</code>","text":"<p>Visualizes annotations on an image.</p> <p>This method takes an image and an instance of sv.Detections, and overlays the specified types of marks (boxes, masks, polygons, labels) on the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The image on which to overlay annotations.</p> required <code>marks</code> <code>Detections</code> <p>The detection results containing the annotations.</p> required <code>with_box</code> <code>bool</code> <p>Whether to draw bounding boxes. Defaults to False.</p> <code>False</code> <code>with_mask</code> <code>bool</code> <p>Whether to overlay masks. Defaults to False.</p> <code>False</code> <code>with_polygon</code> <code>bool</code> <p>Whether to draw polygons. Defaults to True.</p> <code>True</code> <code>with_label</code> <code>bool</code> <p>Whether to add labels. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The annotated image.</p> Source code in <code>maestro/visualizers.py</code> <pre><code>def visualize(\n    self,\n    image: np.ndarray,\n    marks: sv.Detections,\n    with_box: bool = False,\n    with_mask: bool = False,\n    with_polygon: bool = True,\n    with_label: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Visualizes annotations on an image.\n\n    This method takes an image and an instance of sv.Detections, and overlays\n    the specified types of marks (boxes, masks, polygons, labels) on the image.\n\n    Parameters:\n        image (np.ndarray): The image on which to overlay annotations.\n        marks (sv.Detections): The detection results containing the annotations.\n        with_box (bool): Whether to draw bounding boxes. Defaults to False.\n        with_mask (bool): Whether to overlay masks. Defaults to False.\n        with_polygon (bool): Whether to draw polygons. Defaults to True.\n        with_label (bool): Whether to add labels. Defaults to True.\n\n    Returns:\n        np.ndarray: The annotated image.\n    \"\"\"\n    annotated_image = image.copy()\n    if with_box:\n        annotated_image = self.box_annotator.annotate(scene=annotated_image, detections=marks)\n    if with_mask:\n        annotated_image = self.mask_annotator.annotate(scene=annotated_image, detections=marks)\n    if with_polygon:\n        annotated_image = self.polygon_annotator.annotate(scene=annotated_image, detections=marks)\n    if with_label:\n        labels = list(map(str, range(len(marks))))\n        annotated_image = self.label_annotator.annotate(scene=annotated_image, detections=marks, labels=labels)\n    return annotated_image\n</code></pre>"}]}