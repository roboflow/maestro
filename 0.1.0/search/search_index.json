{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Maestro","text":"maestro <p>coming: when it's ready...</p> <p>maestro is a tool designed to streamline and accelerate the fine-tuning process for multimodal models. It provides ready-to-use recipes for fine-tuning popular vision-language models (VLMs) such as Florence-2, PaliGemma, and Qwen2-VL on downstream vision-language tasks.</p>"},{"location":"#install","title":"install","text":"<p>Pip install the supervision package in a Python&gt;=3.8 environment.</p> <pre><code>pip install maestro\n</code></pre>"},{"location":"#quickstart","title":"quickstart","text":""},{"location":"#cli","title":"CLI","text":"<p>VLMs can be fine-tuned on downstream tasks directly from the command line with <code>maestro</code> command:</p> <pre><code>maestro florence2 train --dataset='&lt;DATASET_PATH&gt;' --epochs=10 --batch-size=8\n</code></pre>"},{"location":"#sdk","title":"SDK","text":"<p>Alternatively, you can fine-tune VLMs using the Python SDK, which accepts the same arguments as the CLI example above:</p> <pre><code>from maestro.trainer.common import MeanAveragePrecisionMetric\nfrom maestro.trainer.models.florence_2 import train, Configuration\n\nconfig = Configuration(\n    dataset='&lt;DATASET_PATH&gt;',\n    epochs=10,\n    batch_size=8,\n    metrics=[MeanAveragePrecisionMetric()]\n)\n\ntrain(config)\n</code></pre>"},{"location":"florence-2/","title":"Florence-2","text":""},{"location":"florence-2/#overview","title":"Overview","text":"<p>Florence-2 is a lightweight vision-language model open-sourced by Microsoft under the MIT license. The model demonstrates strong zero-shot and fine-tuning capabilities across tasks such as captioning, object detection, grounding, and segmentation.</p> <p>Florence-2: Fine-tune Microsoft\u2019s Multimodal Model.</p>"},{"location":"florence-2/#architecture","title":"Architecture","text":"<p>The model takes images and task prompts as input, generating the desired results in text format. It uses a DaViT vision encoder to convert images into visual token embeddings. These are then concatenated with BERT-generated text embeddings and processed by a transformer-based multi-modal encoder-decoder to generate the response.</p> <p> Overview of Florence-2 architecture. Source: Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.</p>"},{"location":"florence-2/#fine-tuning-examples","title":"Fine-tuning Examples","text":""},{"location":"florence-2/#dataset-format","title":"Dataset Format","text":"<p>The Florence-2 model expects a specific dataset structure for training and evaluation. The dataset should be organized into train, test, and validation splits, with each split containing image files and an <code>annotations.jsonl</code> file.</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 123e4567-e89b-12d3-a456-426614174000.png\n\u2502   \u251c\u2500\u2500 987f6543-a21c-43c3-a562-926514273001.png\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 annotations.jsonl\n\u251c\u2500\u2500 test/\n\u2502   \u251c\u2500\u2500 456b7890-e32d-44f5-b678-724564172002.png\n\u2502   \u251c\u2500\u2500 678c1234-e45b-67f6-c789-813264172003.png\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 annotations.jsonl\n\u2514\u2500\u2500 valid/\n    \u251c\u2500\u2500 789d2345-f67c-89d7-e891-912354172004.png\n    \u251c\u2500\u2500 135e6789-d89f-12e3-f012-456464172005.png\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 annotations.jsonl\n</code></pre> <p>Depending on the vision task being performed, the structure of the <code>annotations.jsonl</code> file will vary slightly.</p> <p>Warning</p> <p>The dataset samples shown below are formatted for improved readability, with each JSON structure spread across multiple lines. In practice, the <code>annotations.jsonl</code> file must contain each JSON object on a single line, without any line breaks between the key-value pairs. Make sure to adhere to this structure to avoid parsing errors during model training.</p> Object DetectionVisual Question Answering (VQA)Object Character Recognition (OCR) <pre><code>{\n    \"image\":\"123e4567-e89b-12d3-a456-426614174000.png\",\n    \"prefix\":\"&lt;OD&gt;\",\n    \"suffix\":\"9 of clubs&lt;loc_138&gt;&lt;loc_100&gt;&lt;loc_470&gt;&lt;loc_448&gt;10 of clubs&lt;loc_388&gt;&lt;loc_145&gt;&lt;loc_670&gt;&lt;loc_453&gt;\"\n}\n{\n    \"image\":\"987f6543-a21c-43c3-a562-926514273001.png\",\n    \"prefix\":\"&lt;OD&gt;\",\n    \"suffix\":\"5 of clubs&lt;loc_554&gt;&lt;loc_2&gt;&lt;loc_763&gt;&lt;loc_467&gt;6 of clubs&lt;loc_399&gt;&lt;loc_79&gt;&lt;loc_555&gt;&lt;loc_466&gt;\"\n}\n...\n</code></pre> <pre><code>{\n    \"image\":\"123e4567-e89b-12d3-a456-426614174000.png\",\n    \"prefix\":\"&lt;VQA&gt; Is the value of Favorable 38 in 2015?\",\n    \"suffix\":\"Yes\"\n}\n{\n    \"image\":\"987f6543-a21c-43c3-a562-926514273001.png\",\n    \"prefix\":\"&lt;VQA&gt; How many values are below 40 in Unfavorable graph?\",\n    \"suffix\":\"6\"\n}\n...\n</code></pre> <pre><code>{\n    \"image\":\"123e4567-e89b-12d3-a456-426614174000.png\",\n    \"prefix\":\"&lt;OCR&gt;\",\n    \"suffix\":\"ke begherte Die mi\"\n}\n{\n    \"image\":\"987f6543-a21c-43c3-a562-926514273001.png\",\n    \"prefix\":\"&lt;OCR&gt;\",\n    \"suffix\":\"mi uort in de middelt\"\n}\n...\n</code></pre>"},{"location":"florence-2/#cli","title":"CLI","text":"<p>Tip</p> <p>Depending on the GPU you are using, you may need to adjust the <code>batch-size</code> to ensure that your model trains within memory limits. For larger GPUs with more memory, you can increase the batch size for better performance.</p> <p>Tip</p> <p>Depending on the vision task you are executing, you may need to select different vision metrics. For example, tasks like object detection typically use <code>mean_average_precision</code>, while VQA and OCR tasks use metrics like <code>word_error_rate</code> and <code>character_error_rate</code>.</p> <p>Tip</p> <p>You may need to use different learning rates depending on the task. We have found that lower learning rates work better for tasks like OCR or VQA, as these tasks require more precision.</p> Object DetectionVisual Question Answering (VQA)Object Character Recognition (OCR) <pre><code>maestro florence2 train --dataset='&lt;DATASET_PATH&gt;' \\\n--epochs=10 --batch-size=8 --lr=5e-6 --metrics=mean_average_precision\n</code></pre> <pre><code>maestro florence2 train --dataset='&lt;DATASET_PATH&gt;' \\\n--epochs=10 --batch-size=8 --lr=1e-6 \\\n--metrics=word_error_rate, character_error_rate\n</code></pre> <pre><code>maestro florence2 train --dataset='&lt;DATASET_PATH&gt;' \\\n--epochs=10 --batch-size=8 --lr=1e-6 \\\n--metrics=word_error_rate, character_error_rate\n</code></pre>"},{"location":"florence-2/#sdk","title":"SDK","text":"Object DetectionVisual Question Answering (VQA)Object Character Recognition (OCR) <pre><code>from maestro.trainer.common import MeanAveragePrecisionMetric\nfrom maestro.trainer.models.florence_2 import train, Configuration\n\nconfig = Configuration(\n    dataset='&lt;DATASET_PATH&gt;',\n    epochs=10,\n    batch_size=8,\n    lr=5e-6,\n    metrics=[MeanAveragePrecisionMetric()]\n)\n\ntrain(config)\n</code></pre> <pre><code>from maestro.trainer.common import WordErrorRateMetric, CharacterErrorRateMetric\nfrom maestro.trainer.models.florence_2 import train, Configuration\n\nconfig = Configuration(\n    dataset='&lt;DATASET_PATH&gt;',\n    epochs=10,\n    batch_size=8,\n    lr=1e-6,\n    metrics=[WordErrorRateMetric(), CharacterErrorRateMetric()]\n)\n\ntrain(config)\n</code></pre> <pre><code>from maestro.trainer.common import WordErrorRateMetric, CharacterErrorRateMetric\nfrom maestro.trainer.models.florence_2 import train, Configuration\n\nconfig = Configuration(\n    dataset='&lt;DATASET_PATH&gt;',\n    epochs=10,\n    batch_size=8,\n    lr=1e-6,\n    metrics=[WordErrorRateMetric(), CharacterErrorRateMetric()]\n)\n\ntrain(config)\n</code></pre>"},{"location":"florence-2/#api","title":"APIConfigurationtrainevaluate","text":"<p>Configuration for a Florence-2 model.</p> <p>This class encapsulates all the parameters needed for training a Florence-2 model, including dataset paths, model specifications, training hyperparameters, and output settings.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>str</code> <p>Path to the dataset used for training.</p> <code>model_id</code> <code>str</code> <p>Identifier for the Florence-2 model.</p> <code>revision</code> <code>str</code> <p>Revision of the model to use.</p> <code>device</code> <code>device</code> <p>Device to use for training.</p> <code>cache_dir</code> <code>Optional[str]</code> <p>Directory to cache the model.</p> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>optimizer</code> <code>Literal['sgd', 'adamw', 'adam']</code> <p>Optimizer to use for training.</p> <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>lr_scheduler</code> <code>Literal['linear', 'cosine', 'polynomial']</code> <p>Learning rate scheduler.</p> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>val_batch_size</code> <code>Optional[int]</code> <p>Batch size for validation.</p> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading.</p> <code>val_num_workers</code> <code>Optional[int]</code> <p>Number of workers for validation data loading.</p> <code>lora_r</code> <code>int</code> <p>Rank of the LoRA update matrices.</p> <code>lora_alpha</code> <code>int</code> <p>Scaling factor for the LoRA update.</p> <code>lora_dropout</code> <code>float</code> <p>Dropout probability for LoRA layers.</p> <code>bias</code> <code>Literal['none', 'all', 'lora_only']</code> <p>Which bias to train.</p> <code>use_rslora</code> <code>bool</code> <p>Whether to use RSLoRA.</p> <code>init_lora_weights</code> <code>Union[bool, LoraInitLiteral]</code> <p>How to initialize LoRA weights.</p> <code>output_dir</code> <code>str</code> <p>Directory to save output files.</p> <code>metrics</code> <code>List[BaseMetric]</code> <p>List of metrics to track during training.</p> Source code in <code>maestro/trainer/models/florence_2/core.py</code> <pre><code>@dataclass(frozen=True)\nclass Configuration:\n    \"\"\"Configuration for a Florence-2 model.\n\n    This class encapsulates all the parameters needed for training a Florence-2 model,\n    including dataset paths, model specifications, training hyperparameters, and output\n    settings.\n\n    Attributes:\n        dataset (str): Path to the dataset used for training.\n        model_id (str): Identifier for the Florence-2 model.\n        revision (str): Revision of the model to use.\n        device (torch.device): Device to use for training.\n        cache_dir (Optional[str]): Directory to cache the model.\n        epochs (int): Number of training epochs.\n        optimizer (Literal[\"sgd\", \"adamw\", \"adam\"]): Optimizer to use for training.\n        lr (float): Learning rate for the optimizer.\n        lr_scheduler (Literal[\"linear\", \"cosine\", \"polynomial\"]): Learning rate\n            scheduler.\n        batch_size (int): Batch size for training.\n        val_batch_size (Optional[int]): Batch size for validation.\n        num_workers (int): Number of workers for data loading.\n        val_num_workers (Optional[int]): Number of workers for validation data loading.\n        lora_r (int): Rank of the LoRA update matrices.\n        lora_alpha (int): Scaling factor for the LoRA update.\n        lora_dropout (float): Dropout probability for LoRA layers.\n        bias (Literal[\"none\", \"all\", \"lora_only\"]): Which bias to train.\n        use_rslora (bool): Whether to use RSLoRA.\n        init_lora_weights (Union[bool, LoraInitLiteral]): How to initialize LoRA\n            weights.\n        output_dir (str): Directory to save output files.\n        metrics (List[BaseMetric]): List of metrics to track during training.\n    \"\"\"\n\n    dataset: str\n    model_id: str = DEFAULT_FLORENCE2_MODEL_ID\n    revision: str = DEFAULT_FLORENCE2_MODEL_REVISION\n    device: torch.device = DEVICE\n    cache_dir: Optional[str] = None\n    epochs: int = 10\n    optimizer: Literal[\"sgd\", \"adamw\", \"adam\"] = \"adamw\"\n    lr: float = 1e-5\n    lr_scheduler: Literal[\"linear\", \"cosine\", \"polynomial\"] = \"linear\"\n    batch_size: int = 4\n    val_batch_size: Optional[int] = None\n    num_workers: int = 0\n    val_num_workers: Optional[int] = None\n    lora_r: int = 8\n    lora_alpha: int = 8\n    lora_dropout: float = 0.05\n    bias: Literal[\"none\", \"all\", \"lora_only\"] = \"none\"\n    use_rslora: bool = True\n    init_lora_weights: Union[bool, LoraInitLiteral] = \"gaussian\"\n    output_dir: str = \"./training/florence-2\"\n    metrics: list[BaseMetric] = field(default_factory=list)\n</code></pre> <p>Train a Florence-2 model using the provided configuration.</p> <p>This function sets up the training environment, prepares the model and data loaders, and runs the training loop. It also handles metric tracking and checkpoint saving.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>The configuration object containing all necessary parameters for training.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported optimizer is specified in the configuration.</p> Source code in <code>maestro/trainer/models/florence_2/core.py</code> <pre><code>def train(config: Configuration) -&gt; None:\n    \"\"\"Train a Florence-2 model using the provided configuration.\n\n    This function sets up the training environment, prepares the model and data loaders,\n    and runs the training loop. It also handles metric tracking and checkpoint saving.\n\n    Args:\n        config (Configuration): The configuration object containing all necessary\n            parameters for training.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If an unsupported optimizer is specified in the configuration.\n    \"\"\"\n    make_it_reproducible(avoid_non_deterministic_algorithms=False)\n    run_dir = create_new_run_directory(\n        base_output_dir=config.output_dir,\n    )\n    config = replace(\n        config,\n        output_dir=run_dir,\n    )\n    checkpoint_manager = CheckpointManager(run_dir)\n\n    processor, model = load_model(\n        model_id_or_path=config.model_id,\n        revision=config.revision,\n        device=config.device,\n        cache_dir=config.cache_dir,\n    )\n    train_loader, val_loader, test_loader = create_data_loaders(\n        dataset_location=config.dataset,\n        train_batch_size=config.batch_size,\n        processor=processor,\n        device=config.device,\n        num_workers=config.num_workers,\n        test_loaders_workers=config.val_num_workers,\n    )\n    peft_model = prepare_peft_model(\n        model=model,\n        r=config.lora_r,\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n        bias=config.bias,\n        use_rslora=config.use_rslora,\n        init_lora_weights=config.init_lora_weights,\n        revision=config.revision,\n    )\n    training_metrics_tracker = MetricsTracker.init(metrics=[\"loss\"])\n    metrics = [\"loss\"]\n    for metric in config.metrics:\n        metrics += metric.describe()\n    validation_metrics_tracker = MetricsTracker.init(metrics=metrics)\n\n    run_training_loop(\n        processor=processor,\n        model=peft_model,\n        data_loaders=(train_loader, val_loader),\n        config=config,\n        training_metrics_tracker=training_metrics_tracker,\n        validation_metrics_tracker=validation_metrics_tracker,\n        checkpoint_manager=checkpoint_manager,\n    )\n\n    save_metric_plots(\n        training_tracker=training_metrics_tracker,\n        validation_tracker=validation_metrics_tracker,\n        output_dir=os.path.join(config.output_dir, \"metrics\"),\n    )\n    training_metrics_tracker.as_json(output_dir=os.path.join(config.output_dir, \"metrics\"), filename=\"training.json\")\n    validation_metrics_tracker.as_json(\n        output_dir=os.path.join(config.output_dir, \"metrics\"), filename=\"validation.json\"\n    )\n\n    # Log out paths for latest and best checkpoints\n    print(f\"Latest checkpoint saved at: {checkpoint_manager.latest_checkpoint_dir}\")\n    print(f\"Best checkpoint saved at: {checkpoint_manager.best_checkpoint_dir}\")\n</code></pre> <p>Evaluate a Florence-2 model using the provided configuration.</p> <p>This function loads the model and data, runs predictions on the evaluation dataset, computes specified metrics, and saves the results.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>The configuration object containing all necessary parameters for evaluation.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>maestro/trainer/models/florence_2/core.py</code> <pre><code>def evaluate(config: Configuration) -&gt; None:\n    \"\"\"Evaluate a Florence-2 model using the provided configuration.\n\n    This function loads the model and data, runs predictions on the evaluation dataset,\n    computes specified metrics, and saves the results.\n\n    Args:\n        config (Configuration): The configuration object containing all necessary\n            parameters for evaluation.\n\n    Returns:\n        None\n    \"\"\"\n    processor, model = load_model(\n        model_id_or_path=config.model_id,\n        revision=config.revision,\n        device=config.device,\n        cache_dir=config.cache_dir,\n    )\n    train_loader, val_loader, test_loader = create_data_loaders(\n        dataset_location=config.dataset,\n        train_batch_size=config.batch_size,\n        processor=processor,\n        device=config.device,\n        num_workers=config.num_workers,\n        test_loaders_workers=config.val_num_workers,\n    )\n    evaluation_loader = test_loader if test_loader is not None else val_loader\n\n    metrics = []\n    for metric in config.metrics:\n        metrics += metric.describe()\n    evaluation_metrics_tracker = MetricsTracker.init(metrics=metrics)\n\n    # Run inference once for all metrics\n    _, expected_answers, generated_answers, images = run_predictions(\n        loader=evaluation_loader, processor=processor, model=model\n    )\n\n    for metric in config.metrics:\n        if isinstance(metric, MeanAveragePrecisionMetric):\n            classes = get_unique_detection_classes(train_loader.dataset)\n            targets, predictions = process_output_for_detection_metric(\n                expected_answers=expected_answers,\n                generated_answers=generated_answers,\n                images=images,\n                classes=classes,\n                processor=processor,\n            )\n            result = metric.compute(targets=targets, predictions=predictions)\n            for key, value in result.items():\n                evaluation_metrics_tracker.register(\n                    metric=key,\n                    epoch=1,\n                    step=1,\n                    value=value,\n                )\n        else:\n            predictions = process_output_for_text_metric(\n                generated_answers=generated_answers,\n                images=images,\n                processor=processor,\n            )\n            result = metric.compute(targets=expected_answers, predictions=predictions)\n            for key, value in result.items():\n                evaluation_metrics_tracker.register(\n                    metric=key,\n                    epoch=1,\n                    step=1,\n                    value=value,\n                )\n\n    evaluation_metrics_tracker.as_json(\n        output_dir=os.path.join(config.output_dir, \"metrics\"), filename=\"evaluation.json\"\n    )\n</code></pre>"},{"location":"metrics/","title":"Metrics","text":"MeanAveragePrecisionMetric <p>               Bases: <code>BaseMetric</code></p> <p>A class used to compute the Mean Average Precision (mAP) metric.</p> <p>mAP is a popular metric for object detection tasks, measuring the average precision across all classes and IoU thresholds.</p> Source code in <code>maestro/trainer/common/utils/metrics.py</code> <pre><code>class MeanAveragePrecisionMetric(BaseMetric):\n    \"\"\"A class used to compute the Mean Average Precision (mAP) metric.\n\n    mAP is a popular metric for object detection tasks, measuring the average precision\n    across all classes and IoU thresholds.\n    \"\"\"\n\n    name = \"mean_average_precision\"\n\n    def describe(self) -&gt; list[str]:\n        \"\"\"Returns a list of metric names that this class will compute.\n\n        Returns:\n            List[str]: A list of metric names.\n        \"\"\"\n        return [\"map50:95\", \"map50\", \"map75\"]\n\n    def compute(self, targets: list[sv.Detections], predictions: list[sv.Detections]) -&gt; dict[str, float]:\n        \"\"\"Computes the mAP metrics based on the targets and predictions.\n\n        Args:\n            targets (List[sv.Detections]): The ground truth detections.\n            predictions (List[sv.Detections]): The predicted detections.\n\n        Returns:\n            Dict[str, float]: A dictionary of computed mAP metrics with metric names as\n                keys and their values.\n        \"\"\"\n        result = MeanAveragePrecision().update(targets=targets, predictions=predictions).compute()\n        return {\"map50:95\": result.map50_95, \"map50\": result.map50, \"map75\": result.map75}\n</code></pre> WordErrorRateMetric <p>               Bases: <code>BaseMetric</code></p> <p>A class used to compute the Word Error Rate (WER) metric.</p> <p>WER measures the edit distance between predicted and reference transcriptions at the word level, commonly used in speech recognition and machine translation.</p> Source code in <code>maestro/trainer/common/utils/metrics.py</code> <pre><code>class WordErrorRateMetric(BaseMetric):\n    \"\"\"A class used to compute the Word Error Rate (WER) metric.\n\n    WER measures the edit distance between predicted and reference transcriptions\n    at the word level, commonly used in speech recognition and machine translation.\n    \"\"\"\n\n    name = \"word_error_rate\"\n\n    def describe(self) -&gt; list[str]:\n        \"\"\"Returns a list of metric names that this class will compute.\n\n        Returns:\n            List[str]: A list of metric names.\n        \"\"\"\n        return [\"wer\"]\n\n    def compute(self, targets: list[str], predictions: list[str]) -&gt; dict[str, float]:\n        \"\"\"Computes the WER metric based on the targets and predictions.\n\n        Args:\n            targets (List[str]): The ground truth texts.\n            predictions (List[str]): The predicted texts.\n\n        Returns:\n            Dict[str, float]: A dictionary of computed WER metrics with metric names as\n                keys and their values.\n        \"\"\"\n        if len(targets) != len(predictions):\n            raise ValueError(\"The number of targets and predictions must be the same.\")\n\n        total_wer = 0.0\n        count = len(targets)\n\n        for target, prediction in zip(targets, predictions):\n            total_wer += wer(target, prediction)\n\n        average_wer = total_wer / count if count &gt; 0 else 0.0\n        return {\"wer\": average_wer}\n</code></pre> CharacterErrorRateMetric <p>               Bases: <code>BaseMetric</code></p> <p>A class used to compute the Character Error Rate (CER) metric.</p> <p>CER is similar to WER but operates at the character level, making it useful for tasks like optical character recognition (OCR) and handwriting recognition.</p> Source code in <code>maestro/trainer/common/utils/metrics.py</code> <pre><code>class CharacterErrorRateMetric(BaseMetric):\n    \"\"\"A class used to compute the Character Error Rate (CER) metric.\n\n    CER is similar to WER but operates at the character level, making it useful for\n    tasks like optical character recognition (OCR) and handwriting recognition.\n    \"\"\"\n\n    name = \"character_error_rate\"\n\n    def describe(self) -&gt; list[str]:\n        \"\"\"Returns a list of metric names that this class will compute.\n\n        Returns:\n            List[str]: A list of metric names.\n        \"\"\"\n        return [\"cer\"]\n\n    def compute(self, targets: list[str], predictions: list[str]) -&gt; dict[str, float]:\n        \"\"\"Computes the CER metric based on the targets and predictions.\n\n        Args:\n            targets (List[str]): The ground truth texts.\n            predictions (List[str]): The predicted texts.\n\n        Returns:\n            Dict[str, float]: A dictionary of computed CER metrics with metric names as\n                keys and their values.\n        \"\"\"\n        if len(targets) != len(predictions):\n            raise ValueError(\"The number of targets and predictions must be the same.\")\n\n        total_cer = 0.0\n        count = len(targets)\n\n        for target, prediction in zip(targets, predictions):\n            total_cer += cer(target, prediction)\n\n        average_cer = total_cer / count if count &gt; 0 else 0.0\n        return {\"cer\": average_cer}\n</code></pre>"},{"location":"metrics/#maestro.trainer.common.MeanAveragePrecisionMetric-functions","title":"Functions","text":""},{"location":"metrics/#maestro.trainer.common.MeanAveragePrecisionMetric.compute","title":"<code>compute(targets, predictions)</code>","text":"<p>Computes the mAP metrics based on the targets and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>List[Detections]</code> <p>The ground truth detections.</p> required <code>predictions</code> <code>List[Detections]</code> <p>The predicted detections.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dict[str, float]: A dictionary of computed mAP metrics with metric names as keys and their values.</p> Source code in <code>maestro/trainer/common/utils/metrics.py</code> <pre><code>def compute(self, targets: list[sv.Detections], predictions: list[sv.Detections]) -&gt; dict[str, float]:\n    \"\"\"Computes the mAP metrics based on the targets and predictions.\n\n    Args:\n        targets (List[sv.Detections]): The ground truth detections.\n        predictions (List[sv.Detections]): The predicted detections.\n\n    Returns:\n        Dict[str, float]: A dictionary of computed mAP metrics with metric names as\n            keys and their values.\n    \"\"\"\n    result = MeanAveragePrecision().update(targets=targets, predictions=predictions).compute()\n    return {\"map50:95\": result.map50_95, \"map50\": result.map50, \"map75\": result.map75}\n</code></pre>"},{"location":"metrics/#maestro.trainer.common.MeanAveragePrecisionMetric.describe","title":"<code>describe()</code>","text":"<p>Returns a list of metric names that this class will compute.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of metric names.</p> Source code in <code>maestro/trainer/common/utils/metrics.py</code> <pre><code>def describe(self) -&gt; list[str]:\n    \"\"\"Returns a list of metric names that this class will compute.\n\n    Returns:\n        List[str]: A list of metric names.\n    \"\"\"\n    return [\"map50:95\", \"map50\", \"map75\"]\n</code></pre>"},{"location":"metrics/#maestro.trainer.common.WordErrorRateMetric-functions","title":"Functions","text":""},{"location":"metrics/#maestro.trainer.common.WordErrorRateMetric.compute","title":"<code>compute(targets, predictions)</code>","text":"<p>Computes the WER metric based on the targets and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>List[str]</code> <p>The ground truth texts.</p> required <code>predictions</code> <code>List[str]</code> <p>The predicted texts.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dict[str, float]: A dictionary of computed WER metrics with metric names as keys and their values.</p> Source code in <code>maestro/trainer/common/utils/metrics.py</code> <pre><code>def compute(self, targets: list[str], predictions: list[str]) -&gt; dict[str, float]:\n    \"\"\"Computes the WER metric based on the targets and predictions.\n\n    Args:\n        targets (List[str]): The ground truth texts.\n        predictions (List[str]): The predicted texts.\n\n    Returns:\n        Dict[str, float]: A dictionary of computed WER metrics with metric names as\n            keys and their values.\n    \"\"\"\n    if len(targets) != len(predictions):\n        raise ValueError(\"The number of targets and predictions must be the same.\")\n\n    total_wer = 0.0\n    count = len(targets)\n\n    for target, prediction in zip(targets, predictions):\n        total_wer += wer(target, prediction)\n\n    average_wer = total_wer / count if count &gt; 0 else 0.0\n    return {\"wer\": average_wer}\n</code></pre>"},{"location":"metrics/#maestro.trainer.common.WordErrorRateMetric.describe","title":"<code>describe()</code>","text":"<p>Returns a list of metric names that this class will compute.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of metric names.</p> Source code in <code>maestro/trainer/common/utils/metrics.py</code> <pre><code>def describe(self) -&gt; list[str]:\n    \"\"\"Returns a list of metric names that this class will compute.\n\n    Returns:\n        List[str]: A list of metric names.\n    \"\"\"\n    return [\"wer\"]\n</code></pre>"},{"location":"metrics/#maestro.trainer.common.CharacterErrorRateMetric-functions","title":"Functions","text":""},{"location":"metrics/#maestro.trainer.common.CharacterErrorRateMetric.compute","title":"<code>compute(targets, predictions)</code>","text":"<p>Computes the CER metric based on the targets and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>List[str]</code> <p>The ground truth texts.</p> required <code>predictions</code> <code>List[str]</code> <p>The predicted texts.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dict[str, float]: A dictionary of computed CER metrics with metric names as keys and their values.</p> Source code in <code>maestro/trainer/common/utils/metrics.py</code> <pre><code>def compute(self, targets: list[str], predictions: list[str]) -&gt; dict[str, float]:\n    \"\"\"Computes the CER metric based on the targets and predictions.\n\n    Args:\n        targets (List[str]): The ground truth texts.\n        predictions (List[str]): The predicted texts.\n\n    Returns:\n        Dict[str, float]: A dictionary of computed CER metrics with metric names as\n            keys and their values.\n    \"\"\"\n    if len(targets) != len(predictions):\n        raise ValueError(\"The number of targets and predictions must be the same.\")\n\n    total_cer = 0.0\n    count = len(targets)\n\n    for target, prediction in zip(targets, predictions):\n        total_cer += cer(target, prediction)\n\n    average_cer = total_cer / count if count &gt; 0 else 0.0\n    return {\"cer\": average_cer}\n</code></pre>"},{"location":"metrics/#maestro.trainer.common.CharacterErrorRateMetric.describe","title":"<code>describe()</code>","text":"<p>Returns a list of metric names that this class will compute.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of metric names.</p> Source code in <code>maestro/trainer/common/utils/metrics.py</code> <pre><code>def describe(self) -&gt; list[str]:\n    \"\"\"Returns a list of metric names that this class will compute.\n\n    Returns:\n        List[str]: A list of metric names.\n    \"\"\"\n    return [\"cer\"]\n</code></pre>"},{"location":"tasks/","title":"Tasks","text":""},{"location":"tasks/#object-detection","title":"Object Detection","text":"<p>Object Detection is a core computer vision task where a model is trained to identify and locate multiple objects within an image by drawing bounding boxes around them. In the context of Vision-Language Models (VLMs), object detection is enhanced by the model's ability to not only recognize objects but also describe them in natural language. VLMs can provide additional context by naming objects, detailing attributes (such as color, size, or type), and offering richer descriptions of the scene. This fusion of vision and language supports more detailed and semantically aware detection, where object recognition can be linked to more complex visual understanding tasks.</p>"},{"location":"tasks/#visual-question-answering-vqa","title":"Visual Question Answering (VQA)","text":"<p>Visual Question Answering (VQA) merges vision and language by requiring a model to analyze an image and answer questions about its content. VLMs excel in VQA because they jointly understand both the visual components of an image and the linguistic details of the question. This allows the model to perform tasks like answering \"How many dogs are there?\" or \"Is the person in the image wearing glasses?\" with high accuracy. VQA is a key task for VLMs, demonstrating their ability to reason about complex visual scenes while considering natural language prompts.</p>"},{"location":"tasks/#object-character-recognition-ocr","title":"Object Character Recognition (OCR)","text":"<p>Object Character Recognition (OCR) involves detecting and recognizing text within an image, often from signs, documents, or other real-world scenes. With VLMs, OCR capabilities go beyond simple text extraction. These models understand the context in which the text appears, enabling them to answer questions, perform translations, or incorporate textual information into broader visual tasks. This contextual awareness makes VLMs particularly adept at handling tasks like reading and interpreting text embedded in images, and answering questions like \"What does the sign say?\" or \"Translate the text in the image.\"</p>"}]}