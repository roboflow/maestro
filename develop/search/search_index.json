{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Maestro","text":"maestro VLM fine-tuning for everyone"},{"location":"#hello","title":"Hello","text":"<p>maestro is a streamlined tool to accelerate the fine-tuning of multimodal models. By encapsulating best practices from our core modules, maestro handles configuration, data loading, reproducibility, and training loop setup. It currently offers ready-to-use recipes for popular vision-language models such as Florence-2, PaliGemma 2, and Qwen2.5-VL.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#install","title":"Install <p>To begin, install the model-specific dependencies. Since some models may have clashing requirements, we recommend creating a dedicated Python environment for each model.</p> Florence-2PaliGemma 2Qwen2.5-VL   <pre><code>pip install maestro[florence_2]\n</code></pre>   <pre><code>pip install maestro[paligemma_2]\n</code></pre>   <pre><code>pip install maestro[qwen_2_5_vl]\npip install git+https://github.com/huggingface/transformers\n</code></pre>  <p>Warning</p> <p>Support for Qwen2.5-VL in transformers is experimental. For now, please install transformers from source to ensure compatibility.</p>","text":""},{"location":"#cli","title":"CLI <p>Kick off fine-tuning with our command-line interface, which leverages the configuration and training routines defined in each model\u2019s core module. Simply specify key parameters such as the dataset location, number of epochs, batch size, optimization strategy, and metrics.</p> Florence-2PaliGemma 2Qwen2.5-VL   <pre><code>maestro florence_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"lora\" \\\n  --metrics \"edit_distance\"\n</code></pre>   <pre><code>maestro paligemma_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre>   <pre><code>maestro qwen_2_5_vl train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre>","text":""},{"location":"#python","title":"Python <p>For greater control, use the Python API to fine-tune your models. Import the train function from the corresponding module and define your configuration in a dictionary. The core modules take care of reproducibility, data preparation, and training setup.</p> Florence-2PaliGemma 2Qwen2.5-VL   <pre><code>from maestro.trainer.models.florence_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>   <pre><code>from maestro.trainer.models.paligemma_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>   <pre><code>from maestro.trainer.models.qwen_2_5_vl.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>","text":""},{"location":"models/florence_2/","title":"Florence-2","text":""},{"location":"models/florence_2/#overview","title":"Overview","text":"<p>Florence-2 is a lightweight vision-language model open-sourced by Microsoft under the MIT license. It offers strong zero-shot and fine-tuning capabilities for tasks such as image captioning, object detection, visual grounding, and segmentation. Despite its compact size, training on the extensive FLD-5B dataset (126 million images and 5.4 billion annotations) enables Florence-2 to perform on par with much larger models like Kosmos-2. You can try out the model via HF Spaces, Google Colab, or our interactive playground.</p>"},{"location":"models/florence_2/#install","title":"Install","text":"<pre><code>pip install maestro[florence_2]\n</code></pre>"},{"location":"models/florence_2/#train","title":"Train","text":"<p>The training routines support various optimization strategies such as LoRA, and freezing the vision encoder. Customize your fine-tuning process via CLI or Python to align with your dataset and task requirements.</p>"},{"location":"models/florence_2/#cli","title":"CLI","text":"<p>Kick off training from the command line by running the command below. Be sure to replace the dataset path and adjust the hyperparameters (such as epochs and batch size) to suit your needs.</p> <pre><code>maestro florence_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"lora\" \\\n  --metrics \"edit_distance\"\n</code></pre>"},{"location":"models/florence_2/#python","title":"Python","text":"<p>For more control, you can fine-tune Florence-2 using the Python API. Create a configuration dictionary with your training parameters and pass it to the train function to integrate the process into your custom workflow.</p> <pre><code>from maestro.trainer.models.florence_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"lora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>"},{"location":"models/florence_2/#load","title":"Load","text":"<p>Load a pre-trained or fine-tuned Florence-2 model along with its processor using the load_model function. Specify your model's path and the desired optimization strategy.</p> <pre><code>from maestro.trainer.models.florence_2.checkpoints import (\n    OptimizationStrategy, load_model)\n\nprocessor, model = load_model(\n    model_id_or_path=\"model/location\",\n    optimization_strategy=OptimizationStrategy.NONE\n)\n</code></pre>"},{"location":"models/florence_2/#predict","title":"Predict","text":"<p>Perform inference with Florence-2 using the predict function. Supply an image and a text prefix to obtain predictions, such as object detection outputs or captions.</p> <pre><code>from maestro.trainer.common.datasets import RoboflowJSONLDataset\nfrom maestro.trainer.models.florence_2.inference import predict\n\nds = RoboflowJSONLDataset(\n    jsonl_file_path=\"dataset/location/test/annotations.jsonl\",\n    image_directory_path=\"dataset/location/test\",\n)\n\nimage, entry = ds[0]\n\npredict(model=model, processor=processor, image=image, prefix=entry[\"prefix\"])\n</code></pre>"},{"location":"models/paligemma_2/","title":"PaliGemma 2","text":""},{"location":"models/paligemma_2/#overview","title":"Overview","text":"<p>PaliGemma 2 is an updated and significantly enhanced version of the original PaliGemma vision-language model (VLM). By combining the efficient SigLIP-So400m vision encoder with the robust Gemma 2 language model, PaliGemma 2 processes images at multiple resolutions and fuses visual and textual inputs to deliver strong performance across diverse tasks such as captioning, visual question answering (VQA), optical character recognition (OCR), object detection, and instance segmentation. Fine-tuning enables users to adapt the model to specific tasks while leveraging its scalable architecture.</p>"},{"location":"models/paligemma_2/#install","title":"Install","text":"<pre><code>pip install maestro[paligemma_2]\n</code></pre>"},{"location":"models/paligemma_2/#train","title":"Train","text":"<p>The training routines support various optimization strategies such as LoRA, QLoRA, and freezing the vision encoder. Customize your fine-tuning process via CLI or Python to align with your dataset and task requirements.</p>"},{"location":"models/paligemma_2/#cli","title":"CLI","text":"<p>Kick off training from the command line by running the command below. Be sure to replace the dataset path and adjust the hyperparameters (such as epochs and batch size) to suit your needs.</p> <pre><code>maestro paligemma_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre>"},{"location":"models/paligemma_2/#python","title":"Python","text":"<p>For more control, you can fine-tune PaliGemma 2 using the Python API. Create a configuration dictionary with your training parameters and pass it to the train function to integrate the process into your custom workflow.</p> <pre><code>from maestro.trainer.models.paligemma_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>"},{"location":"models/paligemma_2/#load","title":"Load","text":"<p>Load a pre-trained or fine-tuned PaliGemma 2 model along with its processor using the load_model function. Specify your model's path and the desired optimization strategy.</p> <pre><code>from maestro.trainer.models.paligemma_2.checkpoints import (\n    OptimizationStrategy, load_model\n)\n\nprocessor, model = load_model(\n    model_id_or_path=\"model/location\",\n    optimization_strategy=OptimizationStrategy.NONE\n)\n</code></pre>"},{"location":"models/paligemma_2/#predict","title":"Predict","text":"<p>Perform inference with PaliGemma 2 using the predict function. Supply an image and a text prefix to obtain predictions, such as object detection outputs or captions.</p> <pre><code>from maestro.trainer.common.datasets import RoboflowJSONLDataset\nfrom maestro.trainer.models.paligemma_2.inference import predict\n\nds = RoboflowJSONLDataset(\n    jsonl_file_path=\"dataset/location/test/annotations.jsonl\",\n    image_directory_path=\"dataset/location/test\",\n)\n\nimage, entry = ds[0]\n\npredict(model=model, processor=processor, image=image, prefix=entry[\"prefix\"])\n</code></pre>"},{"location":"models/qwen_2_5_vl/","title":"Qwen2.5-VL","text":""},{"location":"models/qwen_2_5_vl/#overview","title":"Overview","text":"<p>Qwen2.5-VL is a cutting-edge vision-language model that integrates powerful visual understanding and advanced language processing in a unified framework. It excels across a range of tasks\u2014from extensive image recognition and precise object grounding to sophisticated text extraction, document parsing, and dynamic video comprehension\u2014making it ideal for both desktop and mobile applications.</p> <p>Building on significant improvements over its predecessor, Qwen2-VL, the Qwen2.5-VL series (including the high-performing 7B-Instruct and the edge-optimized 3B variants) sets new standards by outperforming models like GPT-4o-mini in various tasks.</p>"},{"location":"models/qwen_2_5_vl/#install","title":"Install","text":"<pre><code>pip install maestro[qwen_2_5_vl]\npip install git+https://github.com/huggingface/transformers\n</code></pre> <p>Warning</p> <p>Support for Qwen2.5-VL in transformers is experimental. Please install transformers from source to ensure compatibility.</p>"},{"location":"models/qwen_2_5_vl/#train","title":"Train","text":"<p>The training routines support various optimization strategies such as LoRA, QLoRA, and freezing the vision encoder. Customize your fine-tuning process via CLI or Python to align with your dataset and task requirements.</p>"},{"location":"models/qwen_2_5_vl/#cli","title":"CLI","text":"<p>Kick off training from the command line by running the command below. Be sure to replace the dataset path and adjust the hyperparameters (such as epochs and batch size) to suit your needs.</p> <pre><code>maestro qwen_2_5_vl train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre>"},{"location":"models/qwen_2_5_vl/#python","title":"Python","text":"<p>For more control, you can fine-tune Qwen2.5-VL using the Python API. Create a configuration dictionary with your training parameters and pass it to the train function to integrate the process into your custom workflow.</p> <pre><code>from maestro.trainer.models.qwen_2_5_vl.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>"},{"location":"models/qwen_2_5_vl/#load","title":"Load","text":"<p>Load a pre-trained or fine-tuned Qwen2.5-VL model along with its processor using the load_model function. Specify your model's path and the desired optimization strategy.</p> <pre><code>from maestro.trainer.models.qwen_2_5_vl.checkpoints import (\n    OptimizationStrategy, load_model\n)\n\nprocessor, model = load_model(\n    model_id_or_path=\"model/location\",\n    optimization_strategy=OptimizationStrategy.NONE\n)\n</code></pre>"},{"location":"models/qwen_2_5_vl/#predict","title":"Predict","text":"<p>Perform inference with Qwen2.5-VL using the predict function. Supply an image and a text prefix to obtain predictions, such as object detection outputs or captions.</p> <pre><code>from maestro.trainer.common.datasets import RoboflowJSONLDataset\nfrom maestro.trainer.models.qwen_2_5_vl.inference import predict\n\nds = RoboflowJSONLDataset(\n    jsonl_file_path=\"dataset/location/test/annotations.jsonl\",\n    image_directory_path=\"dataset/location/test\",\n)\n\nimage, entry = ds[0]\n\npredict(model=model, processor=processor, image=image, prefix=entry[\"prefix\"])\n</code></pre>"}]}