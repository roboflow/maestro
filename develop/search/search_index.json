{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"maestro VLM fine-tuning for everyone"},{"location":"#hello","title":"Hello","text":"<p>maestro is a streamlined tool to accelerate the fine-tuning of multimodal models. By encapsulating best practices from our core modules, maestro handles configuration, data loading, reproducibility, and training loop setup. It currently offers ready-to-use recipes for popular vision-language models such as Florence-2, PaliGemma 2, and Qwen2.5-VL.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#install","title":"Install <p>To begin, install the model-specific dependencies. Since some models may have clashing requirements, we recommend creating a dedicated Python environment for each model.</p> Florence-2PaliGemma 2Qwen2.5-VL   <pre><code>pip install \"maestro[florence_2]\"\n</code></pre>   <pre><code>pip install \"maestro[paligemma_2]\"\n</code></pre>   <pre><code>pip install \"maestro[qwen_2_5_vl]\"\npip install git+https://github.com/huggingface/transformers\n</code></pre>  <p>Warning</p> <p>Support for Qwen2.5-VL in transformers is experimental. For now, please install transformers from source to ensure compatibility.</p>","text":""},{"location":"#cli","title":"CLI <p>Kick off fine-tuning with our command-line interface, which leverages the configuration and training routines defined in each model\u2019s core module. Simply specify key parameters such as the dataset location, number of epochs, batch size, optimization strategy, and metrics.</p> Florence-2PaliGemma 2Qwen2.5-VL   <pre><code>maestro florence_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"lora\" \\\n  --metrics \"edit_distance\"\n</code></pre>   <pre><code>maestro paligemma_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre>   <pre><code>maestro qwen_2_5_vl train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre>","text":""},{"location":"#python","title":"Python <p>For greater control, use the Python API to fine-tune your models. Import the train function from the corresponding module and define your configuration in a dictionary. The core modules take care of reproducibility, data preparation, and training setup.</p> Florence-2PaliGemma 2Qwen2.5-VL   <pre><code>from maestro.trainer.models.florence_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>   <pre><code>from maestro.trainer.models.paligemma_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>   <pre><code>from maestro.trainer.models.qwen_2_5_vl.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>","text":""},{"location":"datasets/jsonl/","title":"JSONL","text":""},{"location":"datasets/jsonl/#overview","title":"Overview","text":"<p>JSONL dataset is a simple, text-based format that makes it easy to work with multimodal data. Each line in a JSONL file is a valid JSON object. Each JSON object must contain the following keys:</p> <ul> <li><code>image</code>: A string specifying the image file name associated with the dataset item.</li> <li><code>prefix</code>: A string representing the prompt that will be sent to the model.</li> <li><code>suffix</code>: A string representing the expected model response.</li> </ul> <p>Warning</p> <p><code>suffix</code> can be as simple as a single number or string, a full paragraph, or even a structured JSON output. Regardless of its content, ensure that the it is properly serialized to conform with JSON value requirements.</p> <p>Tip</p> <p>Use Roboflow's tools to annotate and export your multimodal datasets in JSONL format, streamlining data preparation for model training.</p>"},{"location":"datasets/jsonl/#dataset-structure","title":"Dataset Structure","text":"<p>Divide your dataset into three subdirectories: <code>train</code>, <code>valid</code>, and <code>test</code>. Each subdirectory should contain its own <code>annotations.jsonl</code> file that holds the annotations for that particular split, along with the corresponding image files. Below is an example of the directory structure:</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 annotations.jsonl\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ... (other image files)\n\u251c\u2500\u2500 valid/\n\u2502   \u251c\u2500\u2500 annotations.jsonl\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ... (other image files)\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 annotations.jsonl\n    \u251c\u2500\u2500 image1.jpg\n    \u251c\u2500\u2500 image2.jpg\n    \u2514\u2500\u2500 ... (other image files)\n</code></pre>"},{"location":"datasets/jsonl/#jsonl-examples","title":"JSONL Examples","text":"<p>JSONL is a versatile format that can represent datasets for a wide range of visual-language tasks. Its flexible structure supports multiple annotation styles, making it an ideal choice for integrating diverse data sources.</p>"},{"location":"datasets/jsonl/#object-character-recognition-ocr","title":"Object Character Recognition (OCR)","text":"<p>OCR extracts textual content from images, converting printed or handwritten text into machine-readable data.</p> <pre><code>{\"image\":\"image1.jpg\",\"prefix\":\"read equation in LATEX\",\"suffix\":\"H = \\\\dot { x } _ { i } \\\\Pi _ { x ^ { i } } + \\\\Pi _ { x ^ { i } } \\\\dot { x } _ { i } ^ { * } + \\\\dot { \\\\psi } _ { i } \\\\Pi _ { \\\\psi _ { i } } - \\\\Pi _ { \\\\psi _ { i } ^ { * } } \\\\dot { \\\\psi } _ { i } ^ { * } +\"}\n{\"image\":\"image2.jpg\",\"prefix\":\"read equation in LATEX\",\"suffix\":\"\\\\psi _ { j } ( C _ { r } ^ { \\\\vee } , t ) = \\\\frac { 4 \\\\sinh 2 j t ( \\\\cosh ( 2 w _ { 1 } t ) \\\\cosh ( 2 w _ { 2 } t ) - \\\\cos ^ { 2 } ( x t ) ) } { \\\\sinh 2 t \\\\cosh h t } .\"}\n{\"image\":\"image3.jpg\",\"prefix\":\"read equation in LATEX\",\"suffix\":\"- \\\\frac { h ^ { 2 } } { 2 \\\\lambda } \\\\int d t d ^ { 2 } x d ^ { 2 } x ^ { \\\\prime } ( { \\\\tilde { J } } _ { k } - \\\\frac { J _ { k } ^ { 0 } } { \\\\rho _ { 0 } } { \\\\tilde { J } } _ { 0 } ) ( t , x ) \\\\Delta ^ { - 1 } ( x - x ^ { \\\\prime } ) ( { \\\\tilde { J } } _ { k } - \\\\frac { J _ { k } ^ { 0 } } { \\\\rho _ { 0 } } { \\\\tilde { J } } _ { 0 } ) ( t , x ^ { \\\\prime } ) .\"}\n</code></pre> <p>prefix:          read equation in LATEX</p> <p>suffix:          H = \\\\dot { x } _ { i } \\\\Pi _ { x ^ { i } } + \\\\Pi _ { x ^ { i } } \\\\dot { x } _ { i } ^ { * } + \\\\dot { \\\\psi } _ { i } \\\\Pi _ { \\\\psi _ { i } } - \\\\Pi _ { \\\\psi _ { i } ^ { * } } \\\\dot { \\\\psi } _ { i } ^ { * } +</p> <p>prefix:          read equation in LATEX</p> <p>suffix:          \\\\psi _ { j } ( C _ { r } ^ { \\\\vee } , t ) = \\\\frac { 4 \\\\sinh 2 j t ( \\\\cosh ( 2 w _ { 1 } t ) \\\\cosh ( 2 w _ { 2 } t ) - \\\\cos ^ { 2 } ( x t ) ) } { \\\\sinh 2 t \\\\cosh h t } .</p> <p>prefix:          read equation in LATEX</p> <p>suffix:          - \\\\frac { h ^ { 2 } } { 2 \\\\lambda } \\\\int d t d ^ { 2 } x d ^ { 2 } x ^ { \\\\prime } ( { \\\\tilde { J } } _ { k } - \\\\frac { J _ { k } ^ { 0 } } { \\\\rho _ { 0 } } { \\\\tilde { J } } _ { 0 } ) ( t , x ) \\\\Delta ^ { - 1 } ( x - x ^ { \\\\prime } ) ( { \\\\tilde { J } } _ { k } - \\\\frac { J _ { k } ^ { 0 } } { \\\\rho _ { 0 } } { \\\\tilde { J } } _ { 0 } ) ( t , x ^ { \\\\prime } ) .</p>"},{"location":"datasets/jsonl/#visual-question-answering-vqa","title":"Visual Question Answering (VQA)","text":"<p>VQA tasks require models to answer natural language questions based on the visual content of an image.</p> <pre><code>{\"image\":\"image1.jpg\",\"prefix\":\"What is the ratio of yes to no?\",\"suffix\":\"1.54\"}\n{\"image\":\"image2.jpg\",\"prefix\":\"What was the leading men's magazine in the UK from April 2019 to March 2020?\",\"suffix\":\"GQ\"}\n{\"image\":\"image3.jpg\",\"prefix\":\"Which country has the greatest increase from 1975 to 1980?\",\"suffix\":\"Taiwan\"}\n</code></pre> <p>prefix:          What is the ratio of yes to no?</p> <p>suffix:          1.54</p> <p>prefix:          What was the leading men's magazine in the UK from April 2019 to March 2020?</p> <p>suffix:          GQ</p> <p>prefix:          Which country has the greatest increase from 1975 to 1980?</p> <p>suffix:          Taiwan</p>"},{"location":"datasets/jsonl/#json-data-extraction","title":"JSON Data Extraction","text":"<p>This task involves identifying and extracting structured information formatted as JSON from images or documents, facilitating seamless integration into data pipelines.</p> <pre><code>{\"image\":\"image1.jpg\",\"prefix\":\"extract document data in JSON format\",\"suffix\":\"{\\\"route\\\": \\\"J414-YG-624\\\",\\\"pallet_number\\\": \\\"17\\\",\\\"delivery_date\\\": \\\"9/18/2024\\\",\\\"load\\\": \\\"1\\\",\\\"dock\\\": \\\"D08\\\",\\\"shipment_id\\\": \\\"P18941494362\\\",\\\"destination\\\": \\\"595 Navarro Radial Suite 559, Port Erika, HI 29655\\\",\\\"asn_number\\\": \\\"4690787672\\\",\\\"salesman\\\": \\\"CAROL FREDERICK\\\",\\\"products\\\": [{\\\"description\\\": \\\"159753 - BOX OF PAPER CUPS\\\",\\\"cases\\\": \\\"32\\\",\\\"sales_units\\\": \\\"8\\\",\\\"layers\\\": \\\"2\\\"},{\\\"description\\\": \\\"583947 - BOX OF CLOTH RAGS\\\",\\\"cases\\\": \\\"8\\\",\\\"sales_units\\\": \\\"2\\\",\\\"layers\\\": \\\"5\\\"},{\\\"description\\\": \\\"357951 - 6PK OF HAND SANITIZER\\\",\\\"cases\\\": \\\"2\\\",\\\"sales_units\\\": \\\"32\\\",\\\"layers\\\": \\\"4\\\"},{\\\"description\\\": \\\"847295 - CASE OF DISPOSABLE CAPS\\\",\\\"cases\\\": \\\"16\\\",\\\"sales_units\\\": \\\"4\\\",\\\"layers\\\": \\\"3\\\"}],\\\"total_cases\\\": \\\"58\\\",\\\"total_units\\\": \\\"46\\\",\\\"total_layers\\\": \\\"14\\\",\\\"printed_date\\\": \\\"12/05/2024 10:14\\\",\\\"page_number\\\": \\\"60\\\"}\"}\n{\"image\":\"image2.jpg\",\"prefix\":\"extract document data in JSON format\",\"suffix\":\"{\\\"route\\\": \\\"V183-RZ-924\\\",\\\"pallet_number\\\": \\\"14\\\",\\\"delivery_date\\\": \\\"5/3/2024\\\",\\\"load\\\": \\\"4\\\",\\\"dock\\\": \\\"D20\\\",\\\"shipment_id\\\": \\\"P29812736099\\\",\\\"destination\\\": \\\"706 Meghan Brooks, Amyberg, IA 67863\\\",\\\"asn_number\\\": \\\"2211190904\\\",\\\"salesman\\\": \\\"RYAN GREEN\\\",\\\"products\\\": [{\\\"description\\\": \\\"293847 - ROLL OF METAL WIRE\\\",\\\"cases\\\": \\\"16\\\",\\\"sales_units\\\": \\\"8\\\",\\\"layers\\\": \\\"4\\\"},{\\\"description\\\": \\\"958273 - CASE OF SPRAY MOPS\\\",\\\"cases\\\": \\\"16\\\",\\\"sales_units\\\": \\\"8\\\",\\\"layers\\\": \\\"3\\\"},{\\\"description\\\": \\\"258963 - CASE OF MULTI-SURFACE SPRAY\\\",\\\"cases\\\": \\\"2\\\",\\\"sales_units\\\": \\\"4\\\",\\\"layers\\\": \\\"2\\\"}],\\\"total_cases\\\": \\\"34\\\",\\\"total_units\\\": \\\"20\\\",\\\"total_layers\\\": \\\"9\\\",\\\"printed_date\\\": \\\"12/05/2024 10:14\\\",\\\"page_number\\\": \\\"91\\\"}\"}\n{\"image\":\"image3.jpg\",\"prefix\":\"extract document data in JSON format\",\"suffix\":\"{\\\"route\\\": \\\"A702-SG-978\\\",\\\"pallet_number\\\": \\\"19\\\",\\\"delivery_date\\\": \\\"4/7/2024\\\",\\\"load\\\": \\\"5\\\",\\\"dock\\\": \\\"D30\\\",\\\"shipment_id\\\": \\\"Y69465838537\\\",\\\"destination\\\": \\\"31976 French Wall, East Kimport, NY 87074\\\",\\\"asn_number\\\": \\\"4432967070\\\",\\\"salesman\\\": \\\"PATRICIA ROSS\\\",\\\"products\\\": [{\\\"description\\\": \\\"384756 - CASE OF BUCKET LIDS\\\",\\\"cases\\\": \\\"32\\\",\\\"sales_units\\\": \\\"4\\\",\\\"layers\\\": \\\"3\\\"},{\\\"description\\\": \\\"384756 - CASE OF BUCKET LIDS\\\",\\\"cases\\\": \\\"8\\\",\\\"sales_units\\\": \\\"32\\\",\\\"layers\\\": \\\"4\\\"},{\\\"description\\\": \\\"958273 - CASE OF SPRAY MOPS\\\",\\\"cases\\\": \\\"32\\\",\\\"sales_units\\\": \\\"2\\\",\\\"layers\\\": \\\"5\\\"},{\\\"description\\\": \\\"345678 - BOX OF DISPOSABLE GLOVES\\\",\\\"cases\\\": \\\"64\\\",\\\"sales_units\\\": \\\"16\\\",\\\"layers\\\": \\\"3\\\"}],\\\"total_cases\\\": \\\"136\\\",\\\"total_units\\\": \\\"54\\\",\\\"total_layers\\\": \\\"15\\\",\\\"printed_date\\\": \\\"11/29/2024 17:03\\\",\\\"page_number\\\": \\\"28\\\"}\"}\n</code></pre> <p>prefix:          extract document data in JSON format</p> <p>suffix:         {\"route\": \"J414-YG-624\",\"pallet_number\": \"17\",\"delivery_date\": \"9/18/2024\",\"load\": \"1\",\"dock\": \"D08\",\"shipment_id\": \"P18941494362\",\"destination\": \"595 Navarro Radial Suite 559, Port Erika, HI 29655\",\"asn_number\": \"4690787672\",\"salesman\": \"CAROL FREDERICK\",\"products\": [{\"description\": \"159753 - BOX OF PAPER CUPS\",\"cases\": \"32\",\"sales_units\": \"8\",\"layers\": \"2\"},{\"description\": \"583947 - BOX OF CLOTH RAGS\",\"cases\": \"8\",\"sales_units\": \"2\",\"layers\": \"5\"},{\"description\": \"357951 - 6PK OF HAND SANITIZER\",\"cases\": \"2\",\"sales_units\": \"32\",\"layers\": \"4\"},{\"description\": \"847295 - CASE OF DISPOSABLE CAPS\",\"cases\": \"16\",\"sales_units\": \"4\",\"layers\": \"3\"}],\"total_cases\": \"58\",\"total_units\": \"46\",\"total_layers\": \"14\",\"printed_date\": \"12/05/2024 10:14\",\"page_number\": \"60\"}</p> <p>prefix:          extract document data in JSON format</p> <p>suffix:          {\"route\": \"V183-RZ-924\",\"pallet_number\": \"14\",\"delivery_date\": \"5/3/2024\",\"load\": \"4\",\"dock\": \"D20\",\"shipment_id\": \"P29812736099\",\"destination\": \"706 Meghan Brooks, Amyberg, IA 67863\",\"asn_number\": \"2211190904\",\"salesman\": \"RYAN GREEN\",\"products\": [{\"description\": \"293847 - ROLL OF METAL WIRE\",\"cases\": \"16\",\"sales_units\": \"8\",\"layers\": \"4\"},{\"description\": \"958273 - CASE OF SPRAY MOPS\",\"cases\": \"16\",\"sales_units\": \"8\",\"layers\": \"3\"},{\"description\": \"258963 - CASE OF MULTI-SURFACE SPRAY\",\"cases\": \"2\",\"sales_units\": \"4\",\"layers\": \"2\"}],\"total_cases\": \"34\",\"total_units\": \"20\",\"total_layers\": \"9\",\"printed_date\": \"12/05/2024 10:14\",\"page_number\": \"91\"}</p> <p>prefix:          extract document data in JSON format</p> <p>suffix:          {\"route\": \"A702-SG-978\",\"pallet_number\": \"19\",\"delivery_date\": \"4/7/2024\",\"load\": \"5\",\"dock\": \"D30\",\"shipment_id\": \"Y69465838537\",\"destination\": \"31976 French Wall, East Kimport, NY 87074\",\"asn_number\": \"4432967070\",\"salesman\": \"PATRICIA ROSS\",\"products\": [{\"description\": \"384756 - CASE OF BUCKET LIDS\",\"cases\": \"32\",\"sales_units\": \"4\",\"layers\": \"3\"},{\"description\": \"384756 - CASE OF BUCKET LIDS\",\"cases\": \"8\",\"sales_units\": \"32\",\"layers\": \"4\"},{\"description\": \"958273 - CASE OF SPRAY MOPS\",\"cases\": \"32\",\"sales_units\": \"2\",\"layers\": \"5\"},{\"description\": \"345678 - BOX OF DISPOSABLE GLOVES\",\"cases\": \"64\",\"sales_units\": \"16\",\"layers\": \"3\"}],\"total_cases\": \"136\",\"total_units\": \"54\",\"total_layers\": \"15\",\"printed_date\": \"11/29/2024 17:03\",\"page_number\": \"28\"}</p>"},{"location":"datasets/jsonl/#object-detection","title":"Object Detection","text":"<p>This task involves detecting and localizing multiple objects within an image by drawing bounding boxes around them. Each Vision-Language Model (VLM) may require a different text representation of these bounding boxes to interpret the spatial data correctly. The annotations below are compatible with PaliGemma and PaliGemma 2.</p> <p>Tip</p> <p>We are rolling out support for COCO and YOLO formats soon, and will handle conversion between bounding box representations and the format required by each supported VLM.</p> <pre><code>{\"image\":\"image1.jpg\",\"prefix\":\"detect figure ; table ; text\",\"suffix\":\"&lt;loc0412&gt;&lt;loc0102&gt;&lt;loc0734&gt;&lt;loc0920&gt; figure ; &lt;loc0744&gt;&lt;loc0102&gt;&lt;loc0861&gt;&lt;loc0920&gt; text ; &lt;loc0246&gt;&lt;loc0102&gt;&lt;loc0404&gt;&lt;loc0920&gt; text ; &lt;loc0085&gt;&lt;loc0102&gt;&lt;loc0244&gt;&lt;loc0920&gt; text\"}\n{\"image\":\"image2.jpg\",\"prefix\":\"detect figure ; table ; text\",\"suffix\":\"&lt;loc0516&gt;&lt;loc0114&gt;&lt;loc0945&gt;&lt;loc0502&gt; text ; &lt;loc0084&gt;&lt;loc0116&gt;&lt;loc0497&gt;&lt;loc0906&gt; figure ; &lt;loc0517&gt;&lt;loc0518&gt;&lt;loc0945&gt;&lt;loc0907&gt; text\"}\n{\"image\":\"image3.jpg\",\"prefix\":\"detect figure ; table ; text\",\"suffix\":\"&lt;loc0784&gt;&lt;loc0174&gt;&lt;loc0936&gt;&lt;loc0848&gt; text ; &lt;loc0538&gt;&lt;loc0174&gt;&lt;loc0679&gt;&lt;loc0848&gt; table ; &lt;loc0280&gt;&lt;loc0177&gt;&lt;loc0533&gt;&lt;loc0847&gt; figure ; &lt;loc0068&gt;&lt;loc0174&gt;&lt;loc0278&gt;&lt;loc0848&gt; figure ; &lt;loc0686&gt;&lt;loc0174&gt;&lt;loc0775&gt;&lt;loc0848&gt; text\"}\n</code></pre> <p>prefix:          detect figure ; table ; text</p> <p>suffix:          &lt;loc0412&gt;&lt;loc0102&gt;&lt;loc0734&gt;&lt;loc0920&gt; figure ;          &lt;loc0744&gt;&lt;loc0102&gt;&lt;loc0861&gt;&lt;loc0920&gt; text ;          &lt;loc0246&gt;&lt;loc0102&gt;&lt;loc0404&gt;&lt;loc0920&gt; text ;          &lt;loc0085&gt;&lt;loc0102&gt;&lt;loc0244&gt;&lt;loc0920&gt; text       </p> <p>prefix:          detect figure ; table ; text</p> <p>suffix:          &lt;loc0516&gt;&lt;loc0114&gt;&lt;loc0945&gt;&lt;loc0502&gt; text ;          &lt;loc0084&gt;&lt;loc0116&gt;&lt;loc0497&gt;&lt;loc0906&gt; figure ;          &lt;loc0517&gt;&lt;loc0518&gt;&lt;loc0945&gt;&lt;loc0907&gt; text       </p> <p>prefix:          detect figure ; table ; text</p> <p>suffix:          &lt;loc0784&gt;&lt;loc0174&gt;&lt;loc0936&gt;&lt;loc0848&gt; text ;          &lt;loc0538&gt;&lt;loc0174&gt;&lt;loc0679&gt;&lt;loc0848&gt; table ;          &lt;loc0280&gt;&lt;loc0177&gt;&lt;loc0533&gt;&lt;loc0847&gt; figure ;          &lt;loc0068&gt;&lt;loc0174&gt;&lt;loc0278&gt;&lt;loc0848&gt; figure ;          &lt;loc0686&gt;&lt;loc0174&gt;&lt;loc0775&gt;&lt;loc0848&gt; text       </p>"},{"location":"models/florence_2/","title":"Florence-2","text":""},{"location":"models/florence_2/#overview","title":"Overview","text":"<p>Florence-2 is a lightweight vision-language model open-sourced by Microsoft under the MIT license. It offers strong zero-shot and fine-tuning capabilities for tasks such as image captioning, object detection, visual grounding, and segmentation. Despite its compact size, training on the extensive FLD-5B dataset (126 million images and 5.4 billion annotations) enables Florence-2 to perform on par with much larger models like Kosmos-2. You can try out the model via HF Spaces, Google Colab, or our interactive playground.</p>"},{"location":"models/florence_2/#install","title":"Install","text":"<pre><code>pip install \"maestro[florence_2]\"\n</code></pre>"},{"location":"models/florence_2/#train","title":"Train","text":"<p>The training routines support various optimization strategies such as LoRA, and freezing the vision encoder. Customize your fine-tuning process via CLI or Python to align with your dataset and task requirements.</p>"},{"location":"models/florence_2/#cli","title":"CLI","text":"<p>Kick off training from the command line by running the command below. Be sure to replace the dataset path and adjust the hyperparameters (such as epochs and batch size) to suit your needs.</p> <pre><code>maestro florence_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"lora\" \\\n  --metrics \"edit_distance\"\n</code></pre>"},{"location":"models/florence_2/#python","title":"Python","text":"<p>For more control, you can fine-tune Florence-2 using the Python API. Create a configuration dictionary with your training parameters and pass it to the train function to integrate the process into your custom workflow.</p> <pre><code>from maestro.trainer.models.florence_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"lora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>"},{"location":"models/florence_2/#load","title":"Load","text":"<p>Load a pre-trained or fine-tuned Florence-2 model along with its processor using the load_model function. Specify your model's path and the desired optimization strategy.</p> <pre><code>from maestro.trainer.models.florence_2.checkpoints import (\n    OptimizationStrategy, load_model)\n\nprocessor, model = load_model(\n    model_id_or_path=\"model/location\",\n    optimization_strategy=OptimizationStrategy.NONE\n)\n</code></pre>"},{"location":"models/florence_2/#predict","title":"Predict","text":"<p>Perform inference with Florence-2 using the predict function. Supply an image and a text prefix to obtain predictions, such as object detection outputs or captions.</p> <pre><code>from maestro.trainer.common.datasets.jsonl import JSONLDataset\nfrom maestro.trainer.models.florence_2.inference import predict\n\nds = JSONLDataset(\n    jsonl_file_path=\"dataset/location/test/annotations.jsonl\",\n    image_directory_path=\"dataset/location/test\",\n)\n\nimage, entry = ds[0]\n\npredict(model=model, processor=processor, image=image, prefix=entry[\"prefix\"])\n</code></pre>"},{"location":"models/paligemma_2/","title":"PaliGemma 2","text":""},{"location":"models/paligemma_2/#overview","title":"Overview","text":"<p>PaliGemma 2 is an updated and significantly enhanced version of the original PaliGemma vision-language model (VLM). By combining the efficient SigLIP-So400m vision encoder with the robust Gemma 2 language model, PaliGemma 2 processes images at multiple resolutions and fuses visual and textual inputs to deliver strong performance across diverse tasks such as captioning, visual question answering (VQA), optical character recognition (OCR), object detection, and instance segmentation. Fine-tuning enables users to adapt the model to specific tasks while leveraging its scalable architecture.</p>"},{"location":"models/paligemma_2/#install","title":"Install","text":"<pre><code>pip install \"maestro[paligemma_2]\"\n</code></pre>"},{"location":"models/paligemma_2/#train","title":"Train","text":"<p>The training routines support various optimization strategies such as LoRA, QLoRA, and freezing the vision encoder. Customize your fine-tuning process via CLI or Python to align with your dataset and task requirements.</p>"},{"location":"models/paligemma_2/#cli","title":"CLI","text":"<p>Kick off training from the command line by running the command below. Be sure to replace the dataset path and adjust the hyperparameters (such as epochs and batch size) to suit your needs.</p> <pre><code>maestro paligemma_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre>"},{"location":"models/paligemma_2/#python","title":"Python","text":"<p>For more control, you can fine-tune PaliGemma 2 using the Python API. Create a configuration dictionary with your training parameters and pass it to the train function to integrate the process into your custom workflow.</p> <pre><code>from maestro.trainer.models.paligemma_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>"},{"location":"models/paligemma_2/#load","title":"Load","text":"<p>Load a pre-trained or fine-tuned PaliGemma 2 model along with its processor using the load_model function. Specify your model's path and the desired optimization strategy.</p> <pre><code>from maestro.trainer.models.paligemma_2.checkpoints import (\n    OptimizationStrategy, load_model\n)\n\nprocessor, model = load_model(\n    model_id_or_path=\"model/location\",\n    optimization_strategy=OptimizationStrategy.NONE\n)\n</code></pre>"},{"location":"models/paligemma_2/#predict","title":"Predict","text":"<p>Perform inference with PaliGemma 2 using the predict function. Supply an image and a text prefix to obtain predictions, such as object detection outputs or captions.</p> <pre><code>from maestro.trainer.common.datasets.jsonl import JSONLDataset\nfrom maestro.trainer.models.paligemma_2.inference import predict\n\nds = JSONLDataset(\n    jsonl_file_path=\"dataset/location/test/annotations.jsonl\",\n    image_directory_path=\"dataset/location/test\",\n)\n\nimage, entry = ds[0]\n\npredict(model=model, processor=processor, image=image, prefix=entry[\"prefix\"])\n</code></pre>"},{"location":"models/qwen_2_5_vl/","title":"Qwen2.5-VL","text":""},{"location":"models/qwen_2_5_vl/#overview","title":"Overview","text":"<p>Qwen2.5-VL is a cutting-edge vision-language model that integrates powerful visual understanding and advanced language processing in a unified framework. It excels across a range of tasks\u2014from extensive image recognition and precise object grounding to sophisticated text extraction, document parsing, and dynamic video comprehension\u2014making it ideal for both desktop and mobile applications.</p> <p>Building on significant improvements over its predecessor, Qwen2-VL, the Qwen2.5-VL series (including the high-performing 7B-Instruct and the edge-optimized 3B variants) sets new standards by outperforming models like GPT-4o-mini in various tasks.</p>"},{"location":"models/qwen_2_5_vl/#install","title":"Install","text":"<pre><code>pip install \"maestro[qwen_2_5_vl]\"\npip install \"git+https://github.com/huggingface/transformers\"\n</code></pre> <p>Warning</p> <p>Support for Qwen2.5-VL in transformers is experimental. Please install transformers from source to ensure compatibility.</p>"},{"location":"models/qwen_2_5_vl/#train","title":"Train","text":"<p>The training routines support various optimization strategies such as LoRA, QLoRA, and freezing the vision encoder. Customize your fine-tuning process via CLI or Python to align with your dataset and task requirements.</p>"},{"location":"models/qwen_2_5_vl/#cli","title":"CLI","text":"<p>Kick off training from the command line by running the command below. Be sure to replace the dataset path and adjust the hyperparameters (such as epochs and batch size) to suit your needs.</p> <pre><code>maestro qwen_2_5_vl train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre>"},{"location":"models/qwen_2_5_vl/#python","title":"Python","text":"<p>For more control, you can fine-tune Qwen2.5-VL using the Python API. Create a configuration dictionary with your training parameters and pass it to the train function to integrate the process into your custom workflow.</p> <pre><code>from maestro.trainer.models.qwen_2_5_vl.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>"},{"location":"models/qwen_2_5_vl/#load","title":"Load","text":"<p>Load a pre-trained or fine-tuned Qwen2.5-VL model along with its processor using the load_model function. Specify your model's path and the desired optimization strategy.</p> <pre><code>from maestro.trainer.models.qwen_2_5_vl.checkpoints import (\n    OptimizationStrategy, load_model\n)\n\nprocessor, model = load_model(\n    model_id_or_path=\"model/location\",\n    optimization_strategy=OptimizationStrategy.NONE\n)\n</code></pre>"},{"location":"models/qwen_2_5_vl/#predict","title":"Predict","text":"<p>Perform inference with Qwen2.5-VL using the predict function. Supply an image and a text prefix to obtain predictions, such as object detection outputs or captions.</p> <pre><code>from maestro.trainer.common.datasets.jsonl import JSONLDataset\nfrom maestro.trainer.models.qwen_2_5_vl.inference import predict\n\nds = JSONLDataset(\n    jsonl_file_path=\"dataset/location/test/annotations.jsonl\",\n    image_directory_path=\"dataset/location/test\",\n)\n\nimage, entry = ds[0]\n\npredict(model=model, processor=processor, image=image, prefix=entry[\"prefix\"])\n</code></pre>"}]}