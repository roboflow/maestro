{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Maestro","text":"maestro"},{"location":"#hello","title":"Hello","text":"<p>maestro is a streamlined tool to accelerate the fine-tuning of multimodal models. By encapsulating best practices from our core modules, maestro handles configuration, data loading, reproducibility, and training loop setup. It currently offers ready-to-use recipes for popular vision-language models such as Florence-2, PaliGemma 2, and Qwen2.5-VL.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#install","title":"Install","text":"<p>To begin, install the model-specific dependencies. Since some models may have clashing requirements, we recommend creating a dedicated Python environment for each model.</p> Florence-2PaliGemma 2Qwen2.5-VL <pre><code>pip install maestro[florence_2]\n</code></pre> <pre><code>pip install maestro[paligemma_2]\n</code></pre> <pre><code>pip install maestro[qwen_2_5_vl]\npip install git+https://github.com/huggingface/transformers\n</code></pre> <p>Warning</p> <p>Support for Qwen2.5-VL in transformers is experimental. For now, please install transformers from source to ensure compatibility.</p>"},{"location":"#cli","title":"CLI","text":"<p>Kick off fine-tuning with our command-line interface, which leverages the configuration and training routines defined in each model\u2019s core module. Simply specify key parameters such as the dataset location, number of epochs, batch size, optimization strategy, and metrics.</p> Florence-2PaliGemma 2Qwen2.5-VL <pre><code>maestro florence_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"lora\" \\\n  --metrics \"edit_distance\"\n</code></pre> <pre><code>maestro paligemma_2 train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre> <pre><code>maestro qwen_2_5_vl train \\\n  --dataset \"dataset/location\" \\\n  --epochs 10 \\\n  --batch-size 4 \\\n  --optimization_strategy \"qlora\" \\\n  --metrics \"edit_distance\"\n</code></pre>"},{"location":"#python","title":"Python","text":"<p>For greater control, use the Python API to fine-tune your models. Import the train function from the corresponding module and define your configuration in a dictionary. The core modules take care of reproducibility, data preparation, and training setup.</p> Florence-2PaliGemma 2Qwen2.5-VL <pre><code>from maestro.trainer.models.florence_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre> <pre><code>from maestro.trainer.models.paligemma_2.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre> <pre><code>from maestro.trainer.models.qwen_2_5_vl.core import train\n\nconfig = {\n    \"dataset\": \"dataset/location\",\n    \"epochs\": 10,\n    \"batch_size\": 4,\n    \"optimization_strategy\": \"qlora\",\n    \"metrics\": [\"edit_distance\"],\n}\n\ntrain(config)\n</code></pre>"},{"location":"florence-2/","title":"Florence 2","text":""},{"location":"florence-2/#overview","title":"Overview","text":"<p>Florence-2 is a lightweight vision-language model open-sourced by Microsoft under the MIT license. The model demonstrates strong zero-shot and fine-tuning capabilities across tasks such as captioning, object detection, grounding, and segmentation.</p> <p>Florence-2: Fine-tune Microsoft\u2019s Multimodal Model.</p>"},{"location":"florence-2/#architecture","title":"Architecture","text":"<p>The model takes images and task prompts as input, generating the desired results in text format. It uses a DaViT vision encoder to convert images into visual token embeddings. These are then concatenated with BERT-generated text embeddings and processed by a transformer-based multi-modal encoder-decoder to generate the response.</p> <p> Overview of Florence-2 architecture. Source: Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.</p>"},{"location":"florence-2/#fine-tuning-examples","title":"Fine-tuning Examples","text":""},{"location":"florence-2/#dataset-format","title":"Dataset Format","text":"<p>The Florence-2 model expects a specific dataset structure for training and evaluation. The dataset should be organized into train, test, and validation splits, with each split containing image files and an <code>annotations.jsonl</code> file.</p> <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 123e4567-e89b-12d3-a456-426614174000.png\n\u2502   \u251c\u2500\u2500 987f6543-a21c-43c3-a562-926514273001.png\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 annotations.jsonl\n\u251c\u2500\u2500 test/\n\u2502   \u251c\u2500\u2500 456b7890-e32d-44f5-b678-724564172002.png\n\u2502   \u251c\u2500\u2500 678c1234-e45b-67f6-c789-813264172003.png\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 annotations.jsonl\n\u2514\u2500\u2500 valid/\n    \u251c\u2500\u2500 789d2345-f67c-89d7-e891-912354172004.png\n    \u251c\u2500\u2500 135e6789-d89f-12e3-f012-456464172005.png\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 annotations.jsonl\n</code></pre> <p>Depending on the vision task being performed, the structure of the <code>annotations.jsonl</code> file will vary slightly.</p> <p>Warning</p> <p>The dataset samples shown below are formatted for improved readability, with each JSON structure spread across multiple lines. In practice, the <code>annotations.jsonl</code> file must contain each JSON object on a single line, without any line breaks between the key-value pairs. Make sure to adhere to this structure to avoid parsing errors during model training.</p> Object DetectionVisual Question Answering (VQA)Object Character Recognition (OCR) <pre><code>{\n    \"image\":\"123e4567-e89b-12d3-a456-426614174000.png\",\n    \"prefix\":\"&lt;OD&gt;\",\n    \"suffix\":\"9 of clubs&lt;loc_138&gt;&lt;loc_100&gt;&lt;loc_470&gt;&lt;loc_448&gt;10 of clubs&lt;loc_388&gt;&lt;loc_145&gt;&lt;loc_670&gt;&lt;loc_453&gt;\"\n}\n{\n    \"image\":\"987f6543-a21c-43c3-a562-926514273001.png\",\n    \"prefix\":\"&lt;OD&gt;\",\n    \"suffix\":\"5 of clubs&lt;loc_554&gt;&lt;loc_2&gt;&lt;loc_763&gt;&lt;loc_467&gt;6 of clubs&lt;loc_399&gt;&lt;loc_79&gt;&lt;loc_555&gt;&lt;loc_466&gt;\"\n}\n...\n</code></pre> <pre><code>{\n    \"image\":\"123e4567-e89b-12d3-a456-426614174000.png\",\n    \"prefix\":\"&lt;VQA&gt; Is the value of Favorable 38 in 2015?\",\n    \"suffix\":\"Yes\"\n}\n{\n    \"image\":\"987f6543-a21c-43c3-a562-926514273001.png\",\n    \"prefix\":\"&lt;VQA&gt; How many values are below 40 in Unfavorable graph?\",\n    \"suffix\":\"6\"\n}\n...\n</code></pre> <pre><code>{\n    \"image\":\"123e4567-e89b-12d3-a456-426614174000.png\",\n    \"prefix\":\"&lt;OCR&gt;\",\n    \"suffix\":\"ke begherte Die mi\"\n}\n{\n    \"image\":\"987f6543-a21c-43c3-a562-926514273001.png\",\n    \"prefix\":\"&lt;OCR&gt;\",\n    \"suffix\":\"mi uort in de middelt\"\n}\n...\n</code></pre>"},{"location":"florence-2/#cli","title":"CLI","text":"<p>Tip</p> <p>Depending on the GPU you are using, you may need to adjust the <code>batch-size</code> to ensure that your model trains within memory limits. For larger GPUs with more memory, you can increase the batch size for better performance.</p> <p>Tip</p> <p>Depending on the vision task you are executing, you may need to select different vision metrics. For example, tasks like object detection typically use <code>mean_average_precision</code>, while VQA and OCR tasks use metrics like <code>word_error_rate</code> and <code>character_error_rate</code>.</p> <p>Tip</p> <p>You may need to use different learning rates depending on the task. We have found that lower learning rates work better for tasks like OCR or VQA, as these tasks require more precision.</p> Object DetectionVisual Question Answering (VQA)Object Character Recognition (OCR) <pre><code>maestro florence2 train --dataset='&lt;DATASET_PATH&gt;' \\\n--epochs=10 --batch-size=8 --lr=5e-6 --metrics=mean_average_precision\n</code></pre> <pre><code>maestro florence2 train --dataset='&lt;DATASET_PATH&gt;' \\\n--epochs=10 --batch-size=8 --lr=1e-6 \\\n--metrics=word_error_rate, character_error_rate\n</code></pre> <pre><code>maestro florence2 train --dataset='&lt;DATASET_PATH&gt;' \\\n--epochs=10 --batch-size=8 --lr=1e-6 \\\n--metrics=word_error_rate, character_error_rate\n</code></pre>"},{"location":"florence-2/#sdk","title":"SDK","text":"Object DetectionVisual Question Answering (VQA)Object Character Recognition (OCR) <pre><code>from maestro.trainer.common import MeanAveragePrecisionMetric\nfrom maestro.trainer.models.florence_2 import train, Configuration\n\nconfig = Configuration(\n    dataset='&lt;DATASET_PATH&gt;',\n    epochs=10,\n    batch_size=8,\n    lr=5e-6,\n    metrics=[MeanAveragePrecisionMetric()]\n)\n\ntrain(config)\n</code></pre> <pre><code>from maestro.trainer.common import WordErrorRateMetric, CharacterErrorRateMetric\nfrom maestro.trainer.models.florence_2 import train, Configuration\n\nconfig = Configuration(\n    dataset='&lt;DATASET_PATH&gt;',\n    epochs=10,\n    batch_size=8,\n    lr=1e-6,\n    metrics=[WordErrorRateMetric(), CharacterErrorRateMetric()]\n)\n\ntrain(config)\n</code></pre> <pre><code>from maestro.trainer.common import WordErrorRateMetric, CharacterErrorRateMetric\nfrom maestro.trainer.models.florence_2 import train, Configuration\n\nconfig = Configuration(\n    dataset='&lt;DATASET_PATH&gt;',\n    epochs=10,\n    batch_size=8,\n    lr=1e-6,\n    metrics=[WordErrorRateMetric(), CharacterErrorRateMetric()]\n)\n\ntrain(config)\n</code></pre>"},{"location":"metrics/","title":"Metrics","text":"MeanAveragePrecisionMetric <p>               Bases: <code>BaseMetric</code></p> <p>A class used to compute the Mean Average Precision (mAP) metric.</p> <p>mAP is a popular metric for object detection tasks, measuring the average precision across all classes and IoU thresholds.</p> Source code in <code>maestro/trainer/common/metrics.py</code> <pre><code>class MeanAveragePrecisionMetric(BaseMetric):\n    \"\"\"A class used to compute the Mean Average Precision (mAP) metric.\n\n    mAP is a popular metric for object detection tasks, measuring the average precision\n    across all classes and IoU thresholds.\n    \"\"\"\n\n    name = \"mean_average_precision\"\n\n    def describe(self) -&gt; list[str]:\n        \"\"\"Returns a list of metric names that this class will compute.\n\n        Returns:\n            List[str]: A list of metric names.\n        \"\"\"\n        return [\"map50:95\", \"map50\", \"map75\"]\n\n    def compute(self, targets: list[sv.Detections], predictions: list[sv.Detections]) -&gt; dict[str, float]:\n        \"\"\"Computes the mAP metrics based on the targets and predictions.\n\n        Args:\n            targets (List[sv.Detections]): The ground truth detections.\n            predictions (List[sv.Detections]): The predicted detections.\n\n        Returns:\n            Dict[str, float]: A dictionary of computed mAP metrics with metric names as\n                keys and their values.\n        \"\"\"\n        result = MeanAveragePrecision().update(targets=targets, predictions=predictions).compute()\n        return {\"map50:95\": result.map50_95, \"map50\": result.map50, \"map75\": result.map75}\n</code></pre>"},{"location":"metrics/#maestro.trainer.common.metrics.MeanAveragePrecisionMetric-functions","title":"Functions","text":""},{"location":"metrics/#maestro.trainer.common.metrics.MeanAveragePrecisionMetric.compute","title":"<code>compute(targets, predictions)</code>","text":"<p>Computes the mAP metrics based on the targets and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>List[Detections]</code> <p>The ground truth detections.</p> required <code>List[Detections]</code> <p>The predicted detections.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dict[str, float]: A dictionary of computed mAP metrics with metric names as keys and their values.</p> Source code in <code>maestro/trainer/common/metrics.py</code> <pre><code>def compute(self, targets: list[sv.Detections], predictions: list[sv.Detections]) -&gt; dict[str, float]:\n    \"\"\"Computes the mAP metrics based on the targets and predictions.\n\n    Args:\n        targets (List[sv.Detections]): The ground truth detections.\n        predictions (List[sv.Detections]): The predicted detections.\n\n    Returns:\n        Dict[str, float]: A dictionary of computed mAP metrics with metric names as\n            keys and their values.\n    \"\"\"\n    result = MeanAveragePrecision().update(targets=targets, predictions=predictions).compute()\n    return {\"map50:95\": result.map50_95, \"map50\": result.map50, \"map75\": result.map75}\n</code></pre>"},{"location":"metrics/#maestro.trainer.common.metrics.MeanAveragePrecisionMetric.compute(targets)","title":"<code>targets</code>","text":""},{"location":"metrics/#maestro.trainer.common.metrics.MeanAveragePrecisionMetric.compute(predictions)","title":"<code>predictions</code>","text":""},{"location":"metrics/#maestro.trainer.common.metrics.MeanAveragePrecisionMetric.describe","title":"<code>describe()</code>","text":"<p>Returns a list of metric names that this class will compute.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of metric names.</p> Source code in <code>maestro/trainer/common/metrics.py</code> <pre><code>def describe(self) -&gt; list[str]:\n    \"\"\"Returns a list of metric names that this class will compute.\n\n    Returns:\n        List[str]: A list of metric names.\n    \"\"\"\n    return [\"map50:95\", \"map50\", \"map75\"]\n</code></pre>"},{"location":"tasks/","title":"Tasks","text":""},{"location":"tasks/#object-detection","title":"Object Detection","text":"<p>Object Detection is a core computer vision task where a model is trained to identify and locate multiple objects within an image by drawing bounding boxes around them. In the context of Vision-Language Models (VLMs), object detection is enhanced by the model's ability to not only recognize objects but also describe them in natural language. VLMs can provide additional context by naming objects, detailing attributes (such as color, size, or type), and offering richer descriptions of the scene. This fusion of vision and language supports more detailed and semantically aware detection, where object recognition can be linked to more complex visual understanding tasks.</p>"},{"location":"tasks/#visual-question-answering-vqa","title":"Visual Question Answering (VQA)","text":"<p>Visual Question Answering (VQA) merges vision and language by requiring a model to analyze an image and answer questions about its content. VLMs excel in VQA because they jointly understand both the visual components of an image and the linguistic details of the question. This allows the model to perform tasks like answering \"How many dogs are there?\" or \"Is the person in the image wearing glasses?\" with high accuracy. VQA is a key task for VLMs, demonstrating their ability to reason about complex visual scenes while considering natural language prompts.</p>"},{"location":"tasks/#object-character-recognition-ocr","title":"Object Character Recognition (OCR)","text":"<p>Object Character Recognition (OCR) involves detecting and recognizing text within an image, often from signs, documents, or other real-world scenes. With VLMs, OCR capabilities go beyond simple text extraction. These models understand the context in which the text appears, enabling them to answer questions, perform translations, or incorporate textual information into broader visual tasks. This contextual awareness makes VLMs particularly adept at handling tasks like reading and interpreting text embedded in images, and answering questions like \"What does the sign say?\" or \"Translate the text in the image.\"</p>"}]}